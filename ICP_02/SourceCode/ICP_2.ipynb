{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-2GNNZieryP",
        "colab_type": "text"
      },
      "source": [
        "**Installation of java and spark**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txS8yQ7Nk4f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGuLMeCIkfMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "99bfc6c5-26a9-4911-f387-427981442b85"
      },
      "source": [
        "!ls /usr/lib/jvm/"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "default-java\t\t   java-11-openjdk-amd64     java-8-openjdk-amd64\n",
            "java-1.11.0-openjdk-amd64  java-1.8.0-openjdk-amd64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmcb62CDe7td",
        "colab_type": "text"
      },
      "source": [
        "**Defining system environments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3RLnQXdkmI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96JisxfCfKL0",
        "colab_type": "text"
      },
      "source": [
        "**Installation of PySpark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umoDFJLtkcWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4a21da3a-6a34-449c-e95a-7d97ac8e1d48"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq-tp8nvfYFS",
        "colab_type": "text"
      },
      "source": [
        "**Importing libraries and Creating SparkSession and SparkContext**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioVQF7cmkBpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"ICP_2\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO2QCoXDh7Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lower_clean_str(x):\n",
        "  '''\n",
        "  Source: https://stackoverflow.com/questions/53218312/pyspark-how-to-remove-punctuation-marks-and-make-lowercase-letters-in-rdd \n",
        "  '''\n",
        "  punc='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "  lowercased_str = x.lower()\n",
        "  for ch in punc:\n",
        "    lowercased_str = lowercased_str.replace(ch, '')\n",
        "  return lowercased_str"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwAz7uoQl7X_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_it_titlecase(x):\n",
        "  '''\n",
        "  It returns the Titlecase of each input\n",
        "  '''  \n",
        "  if len(x.strip()) > 0:\n",
        "    return x[0].upper()+x[1:].lower()\n",
        "  return x"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZytkWILsgxXj",
        "colab_type": "text"
      },
      "source": [
        "**Write a spark program to group the words in a given text file based on their starting letters.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QElSOS_8g8S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the file\n",
        "file_path = '/content/drive/My Drive/BigDataAnalyticsAndApplications/Classroom/icp-2-acikgozmehmet/ICP_Materials/icp2.txt'\n",
        "rdd = sc.textFile(file_path)\n",
        "\n",
        "# Cleaning the punctuation marks and some special characters from the text\n",
        "cleaned_rdd = rdd.map(lower_clean_str)\n",
        "\n",
        "# Creating (Letter, word) tuples for each word\n",
        "tuples_rdd = cleaned_rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: (word[0].upper(), make_it_titlecase(word)))\n",
        "\n",
        "# Creating (Letter, set of words that start with the letter)\n",
        "resultAsSet = tuples_rdd.groupByKey().sortByKey().map(lambda word: ([word[0]] + list(set(word[1]))))\n",
        "\n",
        "# Creating (Letter, List of words that start with the letter)\n",
        "resultAsList = tuples_rdd.groupByKey().sortByKey().map(lambda word: ([word[0]] + list(word[1])))\n",
        "\n",
        "# Saving the output as Set\n",
        "resultAsSet.coalesce(1).saveAsTextFile('/content/drive/My Drive/BigDataAnalyticsAndApplications/Classroom/icp-2-acikgozmehmet/ICP_Materials/outputAsSet')\n",
        "\n",
        "# Saving the output as List\n",
        "resultAsList.coalesce(1).saveAsTextFile('/content/drive/My Drive/BigDataAnalyticsAndApplications/Classroom/icp-2-acikgozmehmet/ICP_Materials/outputAsList')\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eV2KiecjPan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 52,
      "outputs": []
    }
  ]
}